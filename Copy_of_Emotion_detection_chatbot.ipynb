{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_G9jIEWCpFQN"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets torch torchvision torchaudio sentencepiece kaggle\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()  # Upload kaggle.json here\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "Smb2qDGLqZ06",
        "outputId": "624d2314-f1dc-40ca-c21d-42128492feb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d86313f7-43bd-4b13-958d-59460d407cd2\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d86313f7-43bd-4b13-958d-59460d407cd2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"aroojkazmi\",\"key\":\"263d2edef21d7bc9d81cad8a26cb4bb4\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls ~/.kaggle\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOCMa0eMrC99",
        "outputId": "4d9f9bf6-0cbe-44e3-f100-becdf4ac4ae1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kaggle.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n"
      ],
      "metadata": {
        "id": "lWGF3tdLrOMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets list\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sARTY_bZrhkp",
        "outputId": "8a9c1728-655e-4b65-d71a-95b3ffca54ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.7.4.2 / client 1.6.17)\n",
            "ref                                                                    title                                                size  lastUpdated          downloadCount  voteCount  usabilityRating  \n",
            "---------------------------------------------------------------------  --------------------------------------------------  -----  -------------------  -------------  ---------  ---------------  \n",
            "atharvasoundankar/chocolate-sales                                      Chocolate Sales Data üìäüç´                              63KB  2025-03-07 05:44:03           5103         67  1.0              \n",
            "abdulmalik1518/mobiles-dataset-2025                                    Mobiles Dataset (2025)                               20KB  2025-02-18 06:50:24          12026        212  1.0              \n",
            "salahuddinahmedshuvo/ecommerce-consumer-behavior-analysis-data         Ecommerce Consumer Behavior Analysis Data            43KB  2025-03-03 13:09:09           2648         43  0.9411765        \n",
            "willianoliveiragibin/pixar-films                                       Pixar Films                                           2KB  2025-03-07 22:43:51            999         23  1.0              \n",
            "atharvasoundankar/big-4-financial-risk-insights-2020-2025              Big 4 Financial Risk  Insights (2020-2025)            3KB  2025-03-07 05:02:58            953         25  1.0              \n",
            "mahmoudelhemaly/students-grading-dataset                               Student Performance & Behavior Dataset              508KB  2025-02-17 17:38:46           8682        142  1.0              \n",
            "shantanugarg274/lung-cancer-prediction-dataset                         Lung Cancer Prediction Dataset                      127KB  2025-02-23 04:33:41           1968         37  0.9411765        \n",
            "atharvasoundankar/global-music-streaming-trends-and-listener-insights  Global Music Streaming Trends & Listener Insights    95KB  2025-03-09 05:12:00            922         21  1.0              \n",
            "atharvasoundankar/viral-social-media-trends-and-engagement-analysis    üöÄ Viral Social Media Trends & Engagement Analysis   105KB  2025-03-10 04:51:48           1603         31  1.0              \n",
            "adilshamim8/student-performance-on-an-entrance-examination             Student Performance on an Entrance Examination        4KB  2025-03-04 00:09:21           1531         36  1.0              \n",
            "saliltirodkar/ice-cream-sales-analysis-temperature-and-weather         Ice cream sales analysis - temperature and weather   514B  2025-03-13 08:24:03            816         22  0.7647059        \n",
            "halaturkialotaibi/coffee-bean-sales-dataset                            Coffee Bean Sales Dataset                             9KB  2025-03-05 00:46:14           1611         31  0.9411765        \n",
            "smayanj/netflix-users-database                                         Netflix Users Database                              354KB  2025-03-08 12:08:09           1060         44  1.0              \n",
            "logiccraftbyhimanshi/e-commerce-analytics-swiggy-zomato-blinkit        E-Commerce Analytics: Swiggy, Zomato, Blinkit         1MB  2025-02-26 16:50:07           1882         30  0.88235295       \n",
            "ignacioazua/life-expectancy                                            Life Expectancy                                       3KB  2025-03-04 06:16:35           1573         32  1.0              \n",
            "anandshaw2001/video-game-sales                                         Video Game Sales                                    381KB  2025-02-23 05:16:04           1494         51  1.0              \n",
            "anandshaw2001/imdb-movies-and-tv-shows                                 IMDb Movies and TV Shows                              2MB  2025-02-22 05:57:52            962         21  1.0              \n",
            "vinothkannaece/sales-dataset                                           sales dataset                                        27KB  2025-02-18 05:13:42           6430         82  1.0              \n",
            "samayashar/startup-growth-and-funding-trends                           Startup Growth & Funding Trends                      12KB  2025-02-25 18:00:32            919         28  1.0              \n",
            "rzgiza/pokdex-for-all-1025-pokemon-w-text-description                  Pok√©dex For All 1025 Pok√©mon (+ text descriptions)   74KB  2025-02-24 05:21:54            769         22  1.0              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d parulpandey/emotion-dataset\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-wj8Nbixidl",
        "outputId": "23467695-eb39-47ad-8d87-dd888aefc3f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.7.4.2 / client 1.6.17)\n",
            "Dataset URL: https://www.kaggle.com/datasets/parulpandey/emotion-dataset\n",
            "License(s): CC0-1.0\n",
            "Downloading emotion-dataset.zip to /content\n",
            "  0% 0.00/715k [00:00<?, ?B/s]\n",
            "100% 715k/715k [00:00<00:00, 118MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/emotion-dataset.zip -d /content/emotion_dataset/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jppk8qdWxviS",
        "outputId": "dcc7cc26-a1c1-45f4-873a-4fe71d0c4c8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/emotion-dataset.zip\n",
            "  inflating: /content/emotion_dataset/test.csv  \n",
            "  inflating: /content/emotion_dataset/training.csv  \n",
            "  inflating: /content/emotion_dataset/validation.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load datasets\n",
        "train_df = pd.read_csv('/content/emotion_dataset/training.csv')\n",
        "val_df = pd.read_csv('/content/emotion_dataset/validation.csv')\n",
        "test_df = pd.read_csv('/content/emotion_dataset/test.csv')\n",
        "\n",
        "# Display first few rows of the training set\n",
        "print(\"Training Data:\")\n",
        "print(train_df.head())\n",
        "\n",
        "# Check column names and dataset information\n",
        "print(\"\\nDataset Info:\")\n",
        "print(train_df.info())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmEqOKCnx4e5",
        "outputId": "48623356-d629-4b64-affa-62d74fd32362"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data:\n",
            "                                                text  label\n",
            "0                            i didnt feel humiliated      0\n",
            "1  i can go from feeling so hopeless to so damned...      0\n",
            "2   im grabbing a minute to post i feel greedy wrong      3\n",
            "3  i am ever feeling nostalgic about the fireplac...      2\n",
            "4                               i am feeling grouchy      3\n",
            "\n",
            "Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 16000 entries, 0 to 15999\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   text    16000 non-null  object\n",
            " 1   label   16000 non-null  int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 250.1+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Define function for tokenizing text\n",
        "def tokenize_text(text_list, max_length=128):\n",
        "    return tokenizer(\n",
        "        text_list,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "# Tokenizing the datasets\n",
        "train_encodings = tokenize_text(train_df['text'].tolist())\n",
        "val_encodings = tokenize_text(val_df['text'].tolist())\n",
        "test_encodings = tokenize_text(test_df['text'].tolist())\n",
        "\n",
        "# Convert labels to tensors\n",
        "train_labels = torch.tensor(train_df['label'].values)\n",
        "val_labels = torch.tensor(val_df['label'].values)\n",
        "test_labels = torch.tensor(test_df['label'].values)\n",
        "\n",
        "print(\"Tokenization complete!\")\n",
        "print(f\"Train Encodings Shape: {train_encodings['input_ids'].shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uj6CZaH8x_QX",
        "outputId": "f51158a1-6137-4d2a-d81f-fa409f02091d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenization complete!\n",
            "Train Encodings Shape: torch.Size([16000, 87])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class EmotionDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "        item[\"labels\"] = self.labels[idx]\n",
        "        return item\n",
        "\n",
        "# Creating dataset objects\n",
        "train_dataset = EmotionDataset(train_encodings, train_labels)\n",
        "val_dataset = EmotionDataset(val_encodings, val_labels)\n",
        "test_dataset = EmotionDataset(test_encodings, test_labels)\n",
        "\n",
        "# Dataloader for batch processing\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "print(\"Dataset and DataLoader created successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMbM2hpjyQ9m",
        "outputId": "990bf028-e964-4df4-d3b0-16a4df47801d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset and DataLoader created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "class EmotionClassifier(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(EmotionClassifier, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        output = self.dropout(pooled_output)\n",
        "        return self.fc(output)\n",
        "\n",
        "# Define model\n",
        "num_classes = len(set(train_labels.tolist()))  # Get number of unique labels\n",
        "model = EmotionClassifier(num_classes).to(device)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "print(\"Model initialized successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpzzLLDjyYFU",
        "outputId": "1e414328-d64d-497d-e7c6-c1e51e4a1370"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Model initialized successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "train_inputs = train_encodings[\"input_ids\"].clone().detach()\n",
        "train_labels = train_labels.clone().detach()\n",
        "\n",
        "# Create TensorDataset\n",
        "train_dataset = TensorDataset(train_inputs, train_labels)\n",
        "\n",
        "# Create DataLoader\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "print(\"Train DataLoader created successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhFgrFCay0gz",
        "outputId": "5c6a0115-64a8-4493-f38d-e4f99955ceec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train DataLoader created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in train_dataloader:\n",
        "    print(batch)\n",
        "    break  # Print only the first batch\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXo21m2m1F9d",
        "outputId": "9e766967-4495-4522-dbc1-8f4edc92cdbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[ 101, 1045, 2514,  ...,    0,    0,    0],\n",
            "        [ 101, 1045, 2572,  ...,    0,    0,    0],\n",
            "        [ 101, 1045, 2031,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [ 101, 1045, 2031,  ...,    0,    0,    0],\n",
            "        [ 101, 1045, 2113,  ...,    0,    0,    0],\n",
            "        [ 101, 1045, 2514,  ...,    0,    0,    0]]), tensor([0, 0, 4, 1, 0, 1, 1, 4, 0, 1, 4, 2, 1, 1, 4, 0])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "import torch\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Sample text data (replace with your dataset)\n",
        "texts = [\"I am happy\", \"I feel sad\", \"This is exciting!\"]\n",
        "\n",
        "# Tokenize\n",
        "encoding = tokenizer(\n",
        "    texts,\n",
        "    padding=True,            # Pad to the longest sequence\n",
        "    truncation=True,         # Truncate if too long\n",
        "    max_length=50,           # Max token length\n",
        "    return_tensors=\"pt\"      # Return PyTorch tensors\n",
        ")\n",
        "\n",
        "input_ids = encoding[\"input_ids\"]\n",
        "attention_mask = encoding[\"attention_mask\"]\n",
        "\n",
        "# Sample labels (Replace with actual labels)\n",
        "labels = torch.tensor([0, 1, 2])  # Example: 0 = happy, 1 = sad, 2 = excited\n",
        "\n",
        "print(\"Tokenization successful!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euTKF4ti1120",
        "outputId": "69d5b7fb-fb45-490d-adb4-7595c961aa50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenization successful!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Create dataset with input_ids, attention_mask, and labels\n",
        "dataset = TensorDataset(input_ids, attention_mask, labels)\n",
        "\n",
        "# Create DataLoader\n",
        "train_dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "print(\"Train DataLoader created successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JD3Cgqw0173X",
        "outputId": "849fbf9b-9742-4bb7-e7e4-29b34c3767a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train DataLoader created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.optim import AdamW\n",
        "import torch.nn as nn\n",
        "\n",
        "def train_model(model, train_dataloader, epochs=3):\n",
        "    model.train()\n",
        "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "        for batch in train_dataloader:\n",
        "            inputs, attention_mask, labels = batch\n",
        "\n",
        "            # Move tensors to device (GPU or CPU)\n",
        "            inputs, attention_mask, labels = inputs.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids=inputs, attention_mask=attention_mask)\n",
        "\n",
        "            # Ensure outputs is a tensor\n",
        "            if isinstance(outputs, tuple):  # If model returns a tuple, extract the first element\n",
        "                outputs = outputs[0]\n",
        "\n",
        "            # Compute loss\n",
        "            loss = loss_fn(outputs, labels)  # No need for `.logits` anymore\n",
        "            loss.backward()\n",
        "\n",
        "            # Optimize\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f\"Epoch {epoch+1} completed!\")\n",
        "\n",
        "    print(\"Training finished!\")\n",
        "\n",
        "# Run training\n",
        "train_model(model, train_dataloader, epochs=3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyzBFJi_2KrJ",
        "outputId": "86b36e79-3247-49ed-a807-f30e5016a9b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "Epoch 1 completed!\n",
            "Epoch 2/3\n",
            "Epoch 2 completed!\n",
            "Epoch 3/3\n",
            "Epoch 3 completed!\n",
            "Training finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def predict_emotion(model, tokenizer, text, device):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize input text\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
        "\n",
        "    # Move input tensors to the same device as the model\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items() if key != \"token_type_ids\"}  # Exclude 'token_type_ids'\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)  # Directly get tensor output\n",
        "\n",
        "    # Ensure outputs is a tensor\n",
        "    if isinstance(outputs, tuple):\n",
        "        outputs = outputs[0]  # Get the first element if it's a tuple\n",
        "\n",
        "    # Convert logits to probabilities and get the predicted label\n",
        "    probs = torch.nn.functional.softmax(outputs, dim=1)\n",
        "    predicted_label = torch.argmax(probs, dim=1).item()\n",
        "\n",
        "    return predicted_label\n",
        "\n",
        "# Example usage\n",
        "text = \"I feel so great today!\"\n",
        "predicted_emotion = predict_emotion(model, tokenizer, text, device)\n",
        "print(f\"Predicted Emotion: {predicted_emotion}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gRjeKPA3Bus",
        "outputId": "baec4fb8-fee5-4ab3-c90d-44565a51f805"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Emotion: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_emotion(model, tokenizer, text, device):\n",
        "    \"\"\"Predicts the emotion of a given text input.\"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items() if key != \"token_type_ids\"}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    logits = outputs.logits if hasattr(outputs, \"logits\") else outputs\n",
        "    probs = torch.nn.functional.softmax(logits, dim=1)\n",
        "    predicted_label = torch.argmax(probs, dim=1).item()\n",
        "\n",
        "    # Debug: Print confidence scores\n",
        "    print(f\"üîç Debug: {probs.cpu().numpy()} ‚Üí Predicted Label: {predicted_label} ({emotion_labels[predicted_label]})\")\n",
        "\n",
        "    return predicted_label\n"
      ],
      "metadata": {
        "id": "BmNQM8ue4pU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotion_labels = {\n",
        "    0: \"Happy üòä\",\n",
        "    1: \"Sad üò¢\",\n",
        "    2: \"Angry üò°\",\n",
        "    3: \"Surprised üò≤\",\n",
        "    4: \"Neutral üòê\",\n",
        "    5: \"Anxious üò∞\"  # If your dataset has an 'Anxious' class, add it\n",
        "}\n"
      ],
      "metadata": {
        "id": "Z3cjKRt25WZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_emotion(model, tokenizer, text, device):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    probs = torch.nn.functional.softmax(outputs.logits, dim=1).cpu().numpy()\n",
        "    predicted_label = probs.argmax(axis=1)[0]\n",
        "\n",
        "    # üîç Debug Output\n",
        "    print(f\"üîç Debug: {probs} ‚Üí Predicted Label: {predicted_label} ({emotion_labels.get(predicted_label, 'Unknown')})\")\n",
        "\n",
        "    return predicted_label\n"
      ],
      "metadata": {
        "id": "uLowVnYa5abk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def load_model():\n",
        "    model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name, ignore_mismatched_sizes=True)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    return model, tokenizer, device\n",
        "\n",
        "def predict_emotion(model, tokenizer, text, device):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    logits = outputs.logits if hasattr(outputs, 'logits') else outputs\n",
        "    probs = F.softmax(logits, dim=1)\n",
        "    predicted_label = torch.argmax(probs, dim=1).item()\n",
        "    return predicted_label, probs.max().item()\n",
        "\n",
        "def get_emotion_response(label):\n",
        "    responses = {\n",
        "        0: (\"Very Negative\", \"I'm here for you. What's on your mind? üíô\"),\n",
        "        1: (\"Negative\", \"It sounds like something's bothering you. Want to talk about it?\"),\n",
        "        2: (\"Neutral\", \"I hear you. How's your day going?\"),\n",
        "        3: (\"Positive\", \"That‚Äôs nice to hear! Keep the good vibes going! üòä\"),\n",
        "        4: (\"Very Positive\", \"Awesome! I'm happy for you! üéâ\")\n",
        "    }\n",
        "    return responses.get(label, (\"Unknown\", \"I'm not sure how you're feeling. Can you describe it another way?\"))\n",
        "\n",
        "def chatbot(model, tokenizer, device):\n",
        "    print(\"üöÄ Emotion Chatbot is running! Type 'exit' to quit.\")\n",
        "    goodbye_phrases = {\"bye\", \"goodbye\", \"exit\", \"see you\", \"quit\", \"end chat\"}\n",
        "    while True:\n",
        "        user_input = input(\"You: \").strip().lower()\n",
        "        if user_input in goodbye_phrases:\n",
        "            print(\"ü§ñ Chatbot: Goodbye! Take care! üëã\")\n",
        "            break\n",
        "        label, confidence = predict_emotion(model, tokenizer, user_input, device)\n",
        "        emotion, response = get_emotion_response(label)\n",
        "        print(f\"üîç Debug: Predicted Emotion: {emotion} (Score: {confidence:.4f})\")\n",
        "        print(f\"ü§ñ Chatbot [{emotion}]: {response}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model, tokenizer, device = load_model()\n",
        "    chatbot(model, tokenizer, device):\n",
        "    print(\"üöÄ Emotion Chatbot is running! Type 'exit' to quit.\")\n",
        "\n",
        "    farewell_phrases = [\"bye\", \"goodbye\", \"see you\", \"thanks for your time\", \"take care\"]\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "\n",
        "        # Convert to lowercase for case-insensitive matching\n",
        "        lower_input = user_input.lower()\n",
        "\n",
        "        # Check if the input contains a farewell phrase\n",
        "        if any(phrase in lower_input for phrase in farewell_phrases):\n",
        "            print(\"ü§ñ Chatbot: It was great talking to you! Take care! üòä\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yJYQhcOBOo3",
        "outputId": "d49d6905-0ac1-42d6-914d-5d6104e5f369"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Emotion Chatbot is running! Type 'exit' to quit.\n",
            "You: i am doing software engineering\n",
            "üîç Debug: Predicted Emotion: Positive (Score: 0.3245)\n",
            "ü§ñ Chatbot [Positive]: That‚Äôs nice to hear! Keep the good vibes going! üòä\n",
            "You: sometime i feel depressed \n",
            "üîç Debug: Predicted Emotion: Negative (Score: 0.4471)\n",
            "ü§ñ Chatbot [Negative]: It sounds like something's bothering you. Want to talk about it?\n",
            "You: i feel i am unlucky \n",
            "üîç Debug: Predicted Emotion: Very Negative (Score: 0.4949)\n",
            "ü§ñ Chatbot [Very Negative]: I'm here for you. What's on your mind? üíô\n",
            "You: i will tell you next thanks for your time bye\n",
            "üîç Debug: Predicted Emotion: Very Positive (Score: 0.5435)\n",
            "ü§ñ Chatbot [Very Positive]: Awesome! I'm happy for you! üéâ\n",
            "You: bye\n",
            "ü§ñ Chatbot: Goodbye! Take care! üëã\n"
          ]
        }
      ]
    }
  ]
}